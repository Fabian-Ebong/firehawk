# ~/.bashrc

# The contents of config.template are modified by update_vars.sh.
# Editing these contents should only be done in secrets/config, and then propogated with 'source ./update_vars.sh'

# WARNING: When editing your config file do not store any secrets / sensitive information here.
# Secrets should only ever be stored in the encrypted secrets file..
# This unencrypted config file is still stored in your private repository and should not be publicly available.

# CONFIG INITIALIZATION #

# Private values must be used as values only (and not in #commented lines), since it is only the values that are kept private.
# Only Commented lines and the variable names / keys are read from this private secrets/config file in your private repo to auto generate the public firehawk/config.template file when running 'source ./update_vars.sh'.

# If these steps are followed then no private values will be or should be propogated into the public repo firehawk/config.template file.
# Before making any commits of config.template to the public firehawk repo ensure there are no secrets / sensitive information contained in a commit.
# Be sure to provide any new variable keys you may end up adding with a commented out description with example dummy values above your actual private value used to assist others.

# Do not put real world sensitive information in the example comments / #commented out lines.

# New comments should be only added in secrets/config as these lines will be propogated into the config.template schema used to initialise any new  secrets/config file for other users of the Firehawk repo.

# BEGIN CONFIGURATION #

# TF_VAR_remote_subnet_cidr:
# This is the IP range (CIDR notation) of your subnet onsite that the firehawkserver vm will reside in, and that other onsite nodes reside in.
# eg: TF_VAR_remote_subnet_cidr=192.168.29.0/24
# This would be the ip range 192.168.29.0 - 192.168.29.255
TF_VAR_remote_subnet_cidr=insertvalue

# TF_VAR_localnas1_private_ip:
# The IP adress of the onsite system with NFS shares to mount.  This is normally your onsite NAS ip.
# eg: TF_VAR_localnas1_private_ip=192.168.29.11
TF_VAR_localnas1_private_ip=insertvalue

# TF_VAR_houdini_license_server_address:
# The ip of the houdini licence server used when in a dev or prod environment.  This will normally be the same ip server used in dev and production to not waste licences. So in a dev environment, you will probably need to reference the same houdini licence server in production.  Ideally, the houdini license server should be a seperate vm to the vagrant host for stability reasons but this is currently untested.  It is recomended that the licence server ip is the vagrant production VM until otherwise tested.  A licence server should rarely need to be touched, but updates to infrastructure could disrupt it if located on the firehawkserver vm.
# eg: TF_VAR_houdini_license_server_address=192.168.29.80
TF_VAR_houdini_license_server_address=insertvalue

# TF_VAR_workstation_address_dev:
# The address of the workstation the artist will operate from onsite.  This is normally a VM for testing.
# eg: TF_VAR_workstation_address_dev=192.168.92.31
TF_VAR_workstation_address_dev=insertvalue

# TF_VAR_workstation_address_prod:
# The address of the workstation the artist will operate from onsite
# eg: TF_VAR_workstation_address_prod=192.168.92.31
TF_VAR_workstation_address_prod=insertvalue

# TF_VAR_production_volume_path:
# The name of the production volume path for onsite workstations/nodes.  /prod is a relative path depending on location, so for cloud based nodes, this would usually refer to the softnas production path.  So far only /prod has been tested.
# default: TF_VAR_production_volume_path=/prod
TF_VAR_production_volume_path=insertvalue

# TF_VAR_remote_production_volume_path:
# The name of the production volume path for the remote volume.  onsite, this would reference the cloud nas.  For cloud nodes, this would reference the onsite NAS.
# default: TF_VAR_remote_production_volume_path=/remote_prod
TF_VAR_remote_production_volume_path=insertvalue

# TF_VAR_localnas1_export_path:
# The name of the production volume path for onsite workstations/nodes.  /prod is a relative path depending on location, so for cloud based nodes, this would usually refer to the softnas production path.  So far only /prod has been tested.
# default: TF_VAR_localnas1_export_path=/prod
TF_VAR_localnas1_export_path=insertvalue

# TF_VAR_localnas1_volume_name:
# The name of the volume
# default: TF_VAR_localnas1_volume_name=prod
TF_VAR_localnas1_volume_name=insertvalue

# TF_VAR_localnas1_remote_mount_path:
# The name of the offsite production path if mounted over vpn for onsite workstations/nodes
# default: TF_VAR_localnas1_remote_mount_path=/prod_remote
TF_VAR_localnas1_remote_mount_path=insertvalue

# TF_VAR_localnas1_path_abs:
# This is the absolute path for the onsite NFS mount at all locations. For example, /mycity_prod should be the same mount path for all locations over vpn.
# eg: TF_VAR_localnas1_path_abs=/mycity_prod
TF_VAR_localnas1_path_abs=insertvalue

# TF_VAR_softnas1_path_abs:
# This is the absolute path for the cloud NFS mount at all locations. For example, /aws_city_prod should be the same mount path for all locations over VPN.
# eg: TF_VAR_softnas1_path_abs=/aws_city_prod
TF_VAR_softnas1_path_abs=insertvalue

# TF_VAR_firehawk_sync_source:
# The location based path to the firehawk houdini tools repo.
# This location is pushed to S3, and when a softnas production volume is created it will be pulled to the cloud NFS share.
# It contains TOPS S3 sync functions which Side FX PDG will require to sync assets to and from S3.
# This should also be synced prior to rendering if any changes to assets occur.
# If you choose to use a live path on disk instead of the repository path default, you should have a production NFS share /prod, ensure you also bind an identical locaiton based mount eg: /mycity_prod
# The data at these two paths is identical, but /mycity_prod allows access to data in another location over VPN without confusion, whereas /prod for any location will always refer to the site based NAS for performance, and it is not garunteed to be identical to the other location without syncing.
# default: TF_VAR_firehawk_sync_source=/ansible/ansible_collections/firehawkvfx/houdini/roles/houdini_openfirehawk_houdini_tools_sync/files/openfirehawk_houdini_tools
TF_VAR_firehawk_sync_source=insertvalue

# TF_VAR_firehawk_houdini_tools:
# The path used in produciton to reference firehawk houdini tools for all locations.  This should normally be in /prod, which is the most performant site based NFS mount for any location.
# default: TF_VAR_firehawk_houdini_tools=/prod/assets/openfirehawk-houdini-tools
TF_VAR_firehawk_houdini_tools=insertvalue

# TF_VAR_firehawk_sync_target:
# The path to push the assets to, so they are available in cloud.
# eg: TF_VAR_firehawk_sync_source=/prod/assets/openfirehawk-houdini-tools
TF_VAR_firehawk_sync_target=insertvalue

# TF_VAR_deadline_version:
# The version of the deadline installer.
# default: TF_VAR_deadline_version=10.1.1.3
TF_VAR_deadline_version=insertvalue

# TF_VAR_deadline_proxy_root_dir_dev:
# This is the address and port for clients to reach the Deadline RCS on the openfirehawk server in a dev environment.
# eg: TF_VAR_deadline_proxy_root_dir_dev=192.168.29.10:4433
TF_VAR_deadline_proxy_root_dir_dev=insertvalue

# TF_VAR_deadline_proxy_root_dir_prod:
# This is the address and port for clients to reach the Deadline Remote Connection Server on the openfirehawk server in a prod environment.
# eg: TF_VAR_deadline_proxy_root_dir_prod=192.168.29.80:4433
TF_VAR_deadline_proxy_root_dir_prod=insertvalue

# TF_VAR_provision_deadline_spot_plugin:
# If enabled, the deadline spot fleet plugin will be automatically configured.  
# Note that because we alter the mongo db, this may not be supported with future versions of deadline.  You may need to disable it and configure the plugin manually in these circumstances.
# default: TF_VAR_provision_deadline_spot_plugin=true
TF_VAR_provision_deadline_spot_plugin=insertvalue
